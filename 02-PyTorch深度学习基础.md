# PyTorch深度学习基础

## 学习心得

由于我也才学PyTorch，之前也没有基础，再有基础也不会超过两年（2017发布），所以大家都是一起学习。  
话虽如此，但是懂深度学习套路的，持开放而非怀疑态度的人还是很占优势的。引用上一篇的观点就是没学过统计、机器学习的就没有包袱，轻装上路。不推荐先阅读西瓜书等传统方法类书籍，先从深度学习入手，后期因为难以突破再翻看传统方法寻找灵感也来得及。

书籍只推荐[《动手学深度学习》(PyTorch版)](https://tangshusen.me/Dive-into-DL-PyTorch)，虽然有点挖墙脚，但是学有余力可以与MXNet对照，同时掌握两个框架。

至于我对Tensorflow的看法：曾经试着学习1.x版本，功能繁多，成功将我劝退。尽管确实要比PyTorch强大，然而精力有限，还是先别试了。


## 示例
以下为卷积示例

![convolution](images/conv1.gif)
```python
>>> import torch
>>> import torch.nn.functional as F
>>> data=torch.Tensor([[1, 1, 1, 0, 0], [0, 1, 1, 1, 0], [0, 0, 1, 1, 1], [0, 0, 1, 1, 0], [0, 1, 1, 0, 0]]).unsqueeze_(0).unsqueeze_(0)
>>> data
tensor([[[[1., 1., 1., 0., 0.],
          [0., 1., 1., 1., 0.],
          [0., 0., 1., 1., 1.],
          [0., 0., 1., 1., 0.],
          [0., 1., 1., 0., 0.]]]])
>>> kernel=torch.Tensor([[1, 0, 1], [0, 1, 0], [1, 0, 1]]).unsqueeze_(0).unsqueeze_(0)
>>> kernel
tensor([[[[1., 0., 1.],
          [0., 1., 0.],
          [1., 0., 1.]]]])
>>> F.conv2d(data, kernel)
tensor([[[[4., 3., 4.],
          [2., 4., 3.],
          [2., 3., 4.]]]])
```


![convolution](images/conv2.gif)
```python
>>> import torch
>>> import torch.nn.functional as F
>>> data=torch.Tensor([
    [[0, 1, 1, 2, 2], [0, 1, 1, 0, 0], [1, 1, 0, 1, 0], [1, 0, 1, 1, 1], [0, 2, 0, 1, 0]],
    [[1, 1, 1, 2, 0], [0, 2, 1, 1, 2], [1, 2, 0, 0, 2], [0, 2, 1, 2, 1], [2, 0, 1, 2, 0]],
    [[2, 0, 2, 0, 2], [0, 0, 1, 2, 1], [1, 0, 2, 2, 1], [2, 0, 2, 0, 0], [0, 0, 1, 1, 2]],
]).unsqueeze_(0)
>>> kernel=torch.Tensor([
    [[[1, 1, -1], [-1, 0, 1], [-1, -1, 0]],
     [[-1, 0, -1], [0, 0, -1], [1, -1, 0]],
     [[0, 1, 0], [1, 0, 1], [0, -1, 1]],],
    [[[-1, -1, 0], [-1, 1, 0], [-1, 1, 0]],
     [[1, -1, 0], [-1, 0, -1], [-1, 0, 0]],
     [[-1, 0, 1], [1, 0, 1], [0, -1, 0]],],
])
>>> F.conv2d(data, kernel, stride=2, padding=1,bias=torch.Tensor([1,0]))
tensor([[[[ 1.,  0., -3.],
          [-6.,  1.,  1.],
          [ 4., -3.,  1.]],

         [[-1., -6., -4.],
          [-2., -3., -4.],
          [-1., -3., -3.]]]])
```
